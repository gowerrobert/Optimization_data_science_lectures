% This file was created with JabRef 2.10b2.
% Encoding: UTF-8


@Article{CaiCandes:2010,
  Title                    = {A Singular Value Thresholding Algorithm for Matrix Completion},
  Author                   = {Cai, Jian-Feng and Cand\`{e}s, Emmanuel J. and Shen, Zuowei},
  Journal                  = {SIAM J. on Optimization},
  Year                     = {2010},

  Month                    = mar,
  Number                   = {4},
  Pages                    = {1956--1982},
  Volume                   = {20},

  Acmid                    = {1898451},
  Address                  = {Philadelphia, PA, USA},
  ISSN                     = {1052-6234},
  Issue_date               = {January 2010},
  Keywords                 = {Lagrange dual function, Uzawa's algorithm, linearized Bregman iteration, matrix completion, nuclear norm minimization, singular value thresholding},
  Numpages                 = {27},
  Owner                    = {robert},
  Publisher                = {Society for Industrial and Applied Mathematics},
  Timestamp                = {2017.09.24}
}

@Article{Gower2015,
  Title                    = {Randomized Iterative Methods for Linear Systems},
  Author                   = {Gower, Robert Mansel and Richt{\'{a}}rik, Peter},
  Journal                  = {SIAM Journal on Matrix Analysis and Applications},
  Year                     = {2015},
  Number                   = {4},
  Pages                    = {1660--1690},
  Volume                   = {36},

  Keywords                 = {domized newton,iterative methods,linear systems,ran-,random pursuit,randomized coordinate descent,randomized fixed point,randomized kaczmarz,stochastic methods},
  Owner                    = {robert},
  Timestamp                = {2016.01.31}
}

@Article{Gower2015c,
  Title                    = {Stochastic Dual Ascent for Solving Linear Systems},
  Author                   = {Gower, Robert M. and Richt{\'{a}}rik, Peter},
  Journal                  = {arXiv:1512.06890},
  Year                     = {2015},

  Abstract                 = {We develop a new randomized iterative algorithm---{\{}$\backslash$em stochastic dual ascent (SDA){\}}---for finding the projection of a given vector onto the solution space of a linear system. The method is dual in nature: with the dual being a non-strongly concave quadratic maximization problem without constraints. In each iteration of SDA, a dual variable is updated by a carefully chosen point in a subspace spanned by the columns of a random matrix drawn independently from a fixed distribution. The distribution plays the role of a parameter of the method. Our complexity results hold for a wide family of distributions of random matrices, which opens the possibility to fine-tune the stochasticity of the method to particular applications. We prove that primal iterates associated with the dual process converge to the projection exponentially fast in expectation, and give a formula and an insightful lower bound for the convergence rate. We also prove that the same rate applies to dual function values, primal function values and the duality gap. Unlike traditional iterative methods, SDA converges under no additional assumptions on the system (e.g., rank, diagonal dominance) beyond consistency. In fact, our lower bound improves as the rank of the system matrix drops. Many existing randomized methods for linear systems arise as special cases of SDA, including randomized Kaczmarz, randomized Newton, randomized coordinate descent, Gaussian descent, and their variants. In special cases where our method specializes to a known algorithm, we either recover the best known rates, or improve upon them. Finally, we show that the framework can be applied to the distributed average consensus problem to obtain an array of new algorithms. The randomized gossip algorithm arises as a special case.},
  Owner                    = {robert},
  Timestamp                = {2016.01.31},
  Url                      = {http://arxiv.org/abs/1512.06890}
}

@Article{KarimiNS16,
  Title                    = {Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-{\L}ojasiewicz Condition},
  Author                   = {Hamed Karimi and Julie Nutini and Mark W. Schmidt},
  Journal                  = {CoRR},
  Year                     = {2016},
  Volume                   = {abs/1608.04636},

  Owner                    = {robert},
  Timestamp                = {2017.09.17}
}

@Article{Strohmer2009,
  Title                    = {A Randomized {K}aczmarz Algorithm with Exponential Convergence},
  Author                   = {Strohmer, Thomas and Vershynin, Roman},
  Journal                  = {Journal of Fourier Analysis and Applications},
  Year                     = {2009},
  Number                   = {2},
  Pages                    = {262--278},
  Volume                   = {15},

  Abstract                 = {The Kaczmarz method for solving linear systems of equations is an iterative algorithm that has found many applications ranging from computer tomography to digital signal processing. Despite the popularity of this method, useful theoretical estimates for its rate of convergence are still scarce. We introduce a randomized version of the Kaczmarz method for consistent, overdetermined linear systems and we prove that it converges with expected exponential rate. Furthermore, this is the first solver whose rate does not depend on the number of equations in the system. The solver does not even need to know the whole system, but only a small random part of it. It thus outperforms all previously known methods on general extremely overdetermined systems. Even for moderately overdetermined systems, numerical simulations as well as theoretical analysis reveal that our algorithm can converge faster than the celebrated conjugate gradient algorithm. Furthermore, our theory and numerical simulations confirm a prediction of Feichtinger et al. in the context of reconstructing bandlimited functions from nonuniform sampling.},
  Keywords                 = {,Convergence rate,Kaczmarz algorithm,Random matrix,Randomized algorithm},
  Owner                    = {rgower},
  Primaryclass             = {math},
  Timestamp                = {2017.03.06}
}

@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_author:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_keywords:}

