\documentclass[11pt]{article}
\usepackage{colordvi,epsfig,amssymb}
\usepackage{amsmath,graphicx,enumerate,geometry,array}

%\usepackage[noanswer]{exercise} % Use this version if you want to hide the answers!
\usepackage{exercise}
\usepackage[firstinits=true,doi=false,isbn=false,url=false,backend=bibtex]{biblatex}	
\bibliography{exe_conv}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\cqd}{\hfill\rule{2mm}{2mm}} % Full little box
\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

\newcommand{\eqdef}{\overset{\text{def}}{=}} 
\newcommand{\diag}{\mbox{diag}} 

\newcommand{\EE}[2]{\mathbb{E}_{#1}\left[#2\right] }
\newcommand{\E}[1]{\mathbb{E}\left[#1\right] } 
\newcommand{\Prb}[1]{\mathbb{P}\left[#1\right] }
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\dotprod}[1]{\left< #1\right>}
\newcommand{\Tr}[1]{\mbox{Tr}\left( #1\right)}
\providecommand{\Null}[1]{\mathbf{Null}\left( #1\right)}
\providecommand{\Rank}[1]{\mathbf{Rank}\left( #1\right)}
\providecommand{\Range}[1]{\mathbf{Range}\left( #1\right)}

\title{Exercise List: Proving convergence of the (Stochastic) Gradient Descent Method for the Least Squares Problem.}
\author{Robert M. Gower.}


\begin{document}
\maketitle
\section{Introduction}
This is an exercise in proving the convergence of iterative optimization methods. We will take a simple case study: solving the linear least squares problem, and prove the linear convergence of the gradient descent method and a variant of the  stochastic gradient descent (SGD) method with importance sampling. This variant of SGD is also known as the randomized Kaczmarz method and the linear convergence we prove in {\bf Exe.2}  was first established in~\cite{Strohmer2009}.


First we introduce some necessary notation.
\paragraph{Notation:} For every $x, y, \in \R^n$ let $\dotprod{x,y} \eqdef x^\top y$ and let $\norm{x}_2 = \sqrt{\dotprod{x,x}}.$ 
Let $\sigma_{\min}(A)$ and $\sigma_{\max}(A)$ be the smallest and largest singular values of $A$ defined by
\begin{equation} \label{eq:sigvals}
\sigma_{\min}(A) \eqdef \min_{x \in \R^n,\, x \neq 0} \frac{\norm{Ax}_2}{\norm{x}_2} \quad \mbox{and} \quad \sigma_{\max}(A) \eqdef \max_{x \in \R^n, \,x \neq 0} \frac{\norm{Ax}_2}{\norm{x}_2} .
\end{equation}
Thus clearly
\begin{equation}\label{eq:induced}
\frac{\norm{Ax}_2^2}{\norm{x}_2^2} \leq \sigma_{\max}(A)^2, \quad \forall x \in \R^n.
\end{equation}
Let $\norm{A}_F^2 \eqdef \Tr{A^\top A}$ denote the Frobenius norm of $A.$ Finally, a result you will need, is that for every symmetric positive semi-definite matrix $G$ the $L2$ induced matrix norm can be equivalently defined by
\begin{equation} \label{eq:inducedG}
\sigma_{\max}(G) = \max_{x \in \R^n,\, x \neq 0} \frac{\dotprod{Gx,x}_2}{\norm{x}_2^2} = \max_{x \in \R^n, \,x \neq 0} \frac{\norm{Gx}_2}{\norm{x}_2}.
\end{equation}
  
  \section{The Linear Least Squares Problem}
Now consider the problem of solving the linear system
\begin{equation} \label{eq:lin}
Ax =b,
\end{equation}
where $A \in \R^{m \times n}$ and $b \in \R^m.$  We assume that there exists a solution to~\eqref{eq:lin}. We also assume that $n \leq m$ and that $A$ has full column rank so that there is a unique solution $x^*\in \R^n$ to~\eqref{eq:lin}.
We can recast~\eqref{eq:lin} as the following \emph{Least Squares} optimization problem
\begin{equation} \label{eq:prob}
 x^* = \arg\min_{x \in \R^n} \left(\tfrac{1}{2}\norm{Ax -b}_2^2 \eqdef f(x)\right).
\end{equation}

  \section{Exercises}
\begin{ExerciseList}
    \Exercise Consider the Gradient descent method 
    \begin{equation} \label{eq:grad}
    x^{t+1} = x^t - \alpha \nabla f(x^t),
    \end{equation}
    where  \begin{equation} \label{eq:alpha}
    \alpha = \frac{1}{\sigma_{\max}(A)^2},\end{equation}
     is a fixed stepsize.
    
    \ExePart \label{I}Show or convince yourself that 
    \begin{equation} \label{eq:spectral}
    \sigma_{\max}(I-\alpha  A^\top A)^2 = 1- \alpha\, \sigma_{\min}(A)^2 = 1- \frac{\sigma_{\min}(A)^2}{\sigma_{\max}(A)^2}.
    \end{equation}
%     If you finish everything else, prove this as well!
    \ExePart \label{II}Calculate the gradient $\nabla f(x)$ of~\eqref{eq:prob} and re-write the iterates~\eqref{eq:grad} with this gradient.
     
    \ExePart \label{III} Show that the iterates~\eqref{eq:grad} converge to $x^*$ according to
    \[\norm{x^{t+1}-x^*}_2^2 \leq  \left(1- \frac{\sigma_{\min}(A)^2}{\sigma_{\max}(A)^2}\right) \norm{x^t -x^*}_2^2,\]
     for all $t.$ 

  \ExeText \emph{Hint 1:} Subtract $x^*$ from both sides of~\eqref{eq:grad} and use the results from the previous two exercises.\\
    \ExeText \emph{Hint 2:} Remember that $b = A x^*$!
   \Answer[ref={I}]
    First note that 
     \begin{eqnarray*} 
     \dotprod{(I-\alpha  A^\top A)x,x} &=& \norm{x}_2^2 - \alpha \norm{Ax}_2^2 \\
     &\overset{\eqref{eq:alpha}}{\geq} & \norm{x}_2^2-\frac{\norm{Ax}_2^2 }{\sigma_{\max}(A)^2} \\
     & \overset{\eqref{eq:induced}}{\geq} & \norm{x}_2^2 - \frac{\sigma_{\max}(A)^2 \norm{x}_2^2}{\sigma_{\max}(A)^2\norm{x}_2^2} =0,
     \end{eqnarray*}
     thus the matrix $(I-\alpha  A^\top A)$ is positive semi-definite and only has non-negative eigenvalues.
Furthermore
     \begin{eqnarray} \label{eq:adh9aj98}
     \frac{\dotprod{(I-\alpha A^\top A)x, x}}{\norm{x}_2^2} & = &1 - \alpha \frac{\dotprod{A^\top A x,x}}{\norm{x}_2^2}.
     \end{eqnarray}
    Since $(I-\alpha  A^\top A)$ is symmetric positive semi-definite we can use~\eqref{eq:inducedG} to calculate the induced norm, thus we have 
     \begin{eqnarray*}
   \sigma_{\max}(I-\alpha A^\top A)^2 & \overset{\eqref{eq:inducedG}+\eqref{eq:adh9aj98}}{=} & \max_{x \in \R^n} \left(1 - \alpha \frac{\dotprod{A^\top A x,x}}{\norm{x}_2^2}\right)\\
     &= & 1 - \alpha\,\min_{x \in \R^n}  \frac{\dotprod{A^\top A x,x}}{\norm{x}_2^2} \\
     &= & 1- \alpha \,\sigma_{\min}(A)^2.
     \end{eqnarray*}
    \Answer[ref={II}] 
       Differentiating we have
    \[\nabla f(x) = A^\top (Ax -b) = A^\top A(x -x^*),\]
    where the last equality follows since $Ax^* =b.$
    Consequently the gradient descent method~\eqref{eq:grad} can be written as
 \begin{equation} \label{eq:asmd90qjf}x^{t+1} = x^t - \alpha  A^\top A(x^t -x^*).\end{equation}
    \Answer[ref={III}]    

    Subtracting $x^*$ from both sides of~\eqref{eq:asmd90qjf} gives
    \[ x^{t+1}-x^* = x^t -x^*- \alpha  A^\top A(x^t -x^*) = (I-\alpha  A^\top A)(x^t -x^*). \]
    Taking norm squared in the above gives 
  \begin{eqnarray*}
  \norm{x^{t+1} -x^*}_2^2 & \overset{\eqref{eq:induced}}{\leq} & \sigma_{\max}\left(I-\alpha  A^\top A\right)^2 \norm{x^t -x^*}_2^2 \\
& \overset{\eqref{eq:spectral}}{=} & (1- \alpha\, \sigma_{\min}(A)^2)  \norm{x^t -x^*}_2^2.
\end{eqnarray*}   
   In particular for $\alpha = \frac{1}{\sigma_{\max}(A)^2}$ the above shows that
  \begin{eqnarray*}
  \norm{x^{t+1} -x^*}_2^2 & \leq & \left(1- \frac{\sigma_{\min}(A)^2}{\sigma_{\max}(A)^2}\right)  \norm{x^t -x^*}_2^2.
\end{eqnarray*}
    
    %%%%%STOCH GRADIENT %%%%%
    \Exercise The least squares problem~\eqref{eq:prob} can be re-written as
    \begin{equation} \label{eq:probsep}
    \min_x \tfrac{1}{2}\norm{Ax-b}_2^2\quad = \quad \min_x \tfrac{1}{2}\sum_{i=1}^m (A_{i:}x - b_i)^2 \quad \eqdef \quad \min_x\tfrac{1}{2}\sum_{i=1}^m f_i(x)
    \end{equation}
    where $f_i(x) = (A_{i:}x - b_i)^2$,\, $A_{i:}$ denotes the $i$th row of $A$ and $b_i$ denotes the $i$th element of $b.$
    Given this \emph{sum of terms} structure in~\eqref{eq:probsep} we can implement the stochastic gradient method as follows.
    From a given $x^0 \in \R^n$,  consider the iterates
\begin{equation}\label{eq:stochgrad} x^{t+1}=x^t - \alpha_j \nabla f_j(x^t),\end{equation}
where 
\begin{equation} \label{eq:alpha2}
\alpha_j =  \frac{1}{\norm{A_{j:}}_2^2},\end{equation}
   and $j$ is a random index chosen from $\{1,\ldots, m\}$ such that for every $i \in \{1,\ldots, m\}$ the probability that
$j =i$ is  given by  $ \frac{\norm{A_{i:}}_2^2}{\norm{A}_F^2}.$ In other words,
    $\mathbb{P}(j=i) = \frac{\norm{A_{i:}}_2^2}{\norm{A}_F^2} $ for all $i \in \{1,\ldots, m \}$. 

 \ExePart \label{I} Show that
 \begin{equation}\label{eq:P} P_j \eqdef \alpha_j A_{j:}^\top A_{j:} = \frac{A_{j:}^\top A_{j:}}{\norm{A_{j:}}_2^2},\end{equation}
is a projection operator which projects orthogonally onto $\Range{A_{j:}}.$ In other words, show that
 \begin{equation}\label{eq:PP} P_jP_j =P_j \quad \mbox{and} \quad (I-P_j)(I-P_j) = I-P_j.\end{equation}
 Furthermore, verify that
\begin{equation}
\label{eq:EP}\E{P_j} = \sum_{i=1}^m \mathbb{P}(j=i) P_i = \frac{A^\top A}{\norm{A}_F^2}.
\end{equation} 


\ExePart \label{II} 
Using analogous techniques from the previous exercise, show that the iterates~\eqref{eq:stochgrad} converge according to
\begin{equation}
\E{\norm{x^{t+1}-x^*}_2^2} \quad \leq  \quad\left(1- \frac{\sigma_{\min}(A)^2}{\norm{A}_F^2} \right) \E{\norm{x^{t}-x^*}_2^2}.
\end{equation}
This is an amazing and recent result~\cite{Strohmer2009}, since it shows that SGD converges exponentially fast despite the fact that the iterates~\eqref{eq:stochgrad} only require access to a single row of $A$ at a time! This result can be extended to any matrix $A$, including rank deficient matrices. Indeed, so long as there exists a solution to~\eqref{eq:lin}, the  iterates~\eqref{eq:stochgrad} converge to the solution of least norm and at rate of $\left(1- \frac{\sigma_{\min}^+(A)^2}{\norm{A}_F^2} \right)$ where $\sigma_{\min}^+(A)$ is the smallest nonzero singular value of $A$~\cite{Gower2015c}. Thus the assumption that $A$ has full column rank is not necessary. These results have also been extended to a general class of methods~\cite{Gower2015}.

\ExePart \label{III} When is this stochastic gradient method~\eqref{eq:stochgrad} \emph{faster} than the gradient descent method~\eqref{eq:grad}?  Note that the each iteration of SGD costs $O(n)$ floating point operations while an iteration of the GD method costs $O(nm)$ floating point operations. What happens if $m$ is very big? What if $\norm{A}_F^2$ is very large?  Discuss this. 
    \Answer[ref={I}] Verify by  most all claim by direct computation. For instances
     \[ \E{P_j} = \sum_{i=1}^m \mathbb{P}(j=i) P_i = \sum_{i=1}^m \frac{\norm{A_{i:}}_2^2}{\norm{A}_F^2} \frac{A_{i:}^\top A_{i:}}{\norm{A_{i:}}_2^2} =  \sum_{i=1}^m\frac{A_{i:}^\top A_{i:}}{\norm{A}_F^2} = \frac{A^\top A}{\norm{A}_F^2}.\]
    
    \Answer[ref={II}]  First note that
 \[\nabla f_j(x^t) = A_{j:}^\top (A_{j:} x -b_j) = A_{j:}^\top A_{j:}(x -x^*).\]
 Using the above and subtracting $x^*$ from both sides of~\eqref{eq:stochgrad} we have
 \begin{eqnarray*}
 x^{t+1} - x^* &=& x^t-x^* - \alpha_j A_{j:}^\top A_{j:}(x^t -x^*)\\
 &\overset{\eqref{eq:alpha2}}{ =} & \left(I-  \frac{A_{j:}^\top A_{j:}}{\norm{A_{j:}}_2^2} \right)(x^t -x^*).
 \end{eqnarray*}
Taking norm squared in the above we have that
  \begin{eqnarray*}
\norm{x^{t+1} - x^*}_2^2 & =&  \norm{\left(I-  \frac{A_{j:}^\top A_{j:}}{\norm{A_{j:}}_2^2} \right)(x^t -x^*)}_2^2\\
&\overset{\eqref{eq:PP}}{=}&  \dotprod{\left(I-  \frac{A_{j:}^\top A_{j:}}{\norm{A_{j:}}_2^2} \right)(x^t -x^*), x^t -x^*}\\
&= & \norm{x^t -x^*}_2^2 - \dotprod{\frac{A_{j:}^\top A_{j:}}{\norm{A_{j:}}_2^2} (x^t -x^*), x^t -x^*}.
 \end{eqnarray*}
 Taking expectation conditioned on $x^t$ in the above gives
   \begin{eqnarray*}
\E{\norm{x^{t+1} - x^*}_2^2\, | \, x^t} & =& \norm{x^t -x^*}_2^2 - \dotprod{\E{\frac{A_{j:}^\top A_{j:}}{\norm{A_{j:}}_2^2} }(x^t -x^*), x^t -x^*}\\
& \overset{\eqref{eq:EP}}{=} & \norm{x^t -x^*}_2^2 - \frac{1}{\norm{A}_F^2}\dotprod{A^\top A(x^t -x^*), x^t -x^*}\\
& \overset{\eqref{eq:sigvals}}{\leq} & \norm{x^t -x^*}_2^2 -  \frac{\sigma_{\min}(A)^2 }{\norm{A}_F^2}\norm{x^t -x^*}_2^2\\
& =& \left(1-  \frac{\sigma_{\min}(A)^2 }{\norm{A}_F^2}\right)\norm{x^t -x^*}_2^2 .
 \end{eqnarray*}
 It remains to take expectation in the above.
     \Answer[ref={III}] ...
\end{ExerciseList}

\printbibliography
\end{document}