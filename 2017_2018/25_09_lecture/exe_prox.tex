\documentclass[11pt]{article}
\usepackage{colordvi,epsfig,amssymb}
\usepackage{amsmath,graphicx,enumerate,geometry,array}

%\usepackage[noanswer]{exercise} % uncomment this to hide answers!
\usepackage{exercise}
\usepackage[firstinits=true,doi=false,isbn=false,url=false,backend=bibtex]{biblatex}	
\bibliography{../exe_conv}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\cqd}{\hfill\rule{2mm}{2mm}} % Full little box
\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

\newcommand{\eqdef}{\overset{\text{def}}{=}} 
\newcommand{\diag}{\mbox{diag}} 

\newcommand{\EE}[2]{\mathbb{E}_{#1}\left[#2\right] }
\newcommand{\E}[1]{\mathbb{E}\left[#1\right] } 
\newcommand{\Prb}[1]{\mathbb{P}\left[#1\right] }
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\dotprod}[1]{\left< #1\right>}
\newcommand{\Tr}[1]{\mbox{Tr}\left( #1\right)}
\providecommand{\Null}[1]{\mathbf{Null}\left( #1\right)}
\providecommand{\Rank}[1]{\mathbf{Rank}\left( #1\right)}
\providecommand{\Range}[1]{\mathbf{Range}\left( #1\right)}

\newtheorem{theorem}{Theorem}[section]
\title{Exercise List: Proximal Operator.}
\author{Robert M. Gower.}


\begin{document}
\maketitle
\section{Introduction}
%
This is an exercise in deducing closed form expressions for proximal operators. In the first part we will show how to deduce that the proximal operator of the L1 norm is the soft-thresholding operator. In the second part we will show the equivalence between the proximal operator of the matrix nuclear norm and the singular value soft-thresholding operator.


First some necessary notation.
\paragraph{Notation:} For every $x, y, \in \R^n$ let $\dotprod{x,y} \eqdef x^\top y$ and let $\norm{x}_2 = \sqrt{\dotprod{x,x}}.$ Let $\sigma(A)=[\sigma_1(A), \ldots, \sigma_n(A)]$ be the singular values of $A$.
%Let $\sigma_{\min}(A)$ and $\sigma_{\max}(A)$ be the smallest and largest singular values of $A$ defined by
%\begin{equation} \label{eq:sigvals}
%\sigma_{\min}(A) \eqdef \min_{x \in \R^n} \frac{\norm{Ax}_2}{\norm{x}_2} \quad \mbox{and} \quad \sigma_{\max}(A) \eqdef \max_{x \in \R^n} \frac{\norm{Ax}_2}{\norm{x}_2} .
%\end{equation}
%Thus clearly
%\begin{equation}\label{eq:induced}
%\frac{\norm{Ax}_2^2}{\norm{x}_2^2} \leq \sigma_{\max}(A)^2, \quad \forall x \in \R^n.
%\end{equation}
Let $\norm{A}_F^2 \eqdef \Tr{A^\top A} = \sum_{ij} A_{ij}^2$ denote the Frobenius norm of $A$ and let $\norm{A}_* = \sum_{i} \sigma_i(A)$ be the nuclear norm.

%Finally, a result you will need, is that for every symmetric positive semi-definite matrix $G$ the $L2$ induced matrix norm can be equivalently defined by
%\begin{equation} \label{eq:inducedG}
%\sigma_{\max}(G) = \max_{x \in \R^n} \frac{\sqrt{\dotprod{Gx,x}_2}}{\norm{x}_2} = \max_{x \in \R^n} \frac{\norm{Gx}_2}{\norm{x}_2}.
%\end{equation}
%  
  \section{Soft Thresholding}
  Let $f: x \in \R^d \rightarrow f(x)$ be a convex function. Consider the proximal operator
  \begin{equation}
  \mbox{prox}_{f}(v) \eqdef \arg\min_x \frac{1}{2}\norm{x-v}_2^2 + f(x).
  \end{equation}

 
\begin{ExerciseList}
    \Exercise  In this exercise we will show step-by-step that the proximal operator of the L1 norm is the soft thresholding operator, that is
  \begin{equation}\label{eq:l1prox}
  \mbox{prox}_{\lambda \norm{w}_1} (v) =  \left(S_{\lambda}(v_1) ,\ldots, S_{\lambda}(v_n)\right),
  \end{equation}
  where
  \begin{equation}\label{eq:softth}
 S_{\lambda}(v)  = 
\begin{cases}
v- \lambda & \mbox{ if } \lambda <v\\
0 & \mbox{ if } -\lambda\leq v\leq \lambda\\
v+\lambda & \mbox{ if } v<  -\lambda.
\end{cases}
  \end{equation}
      \newpage
  \ExePart \label{I} Show that if $f(x)$ is separable, that is, if $f(x) = \sum_{i=1}^d f_i(x_i)$ then 
  \begin{equation}
  \mbox{prox}_{f}(v) = (\mbox{prox}_{f_1}(v_1),\ldots, \mbox{prox}_{f_d}(v_d)).
  \end{equation}
Consequently  
\[  \mbox{prox}_{\lambda \norm{w}_1} (v)  =  (\mbox{prox}_{\lambda|w_1|}(v_1),\ldots, \mbox{prox}_{\lambda|w_d|}(v_d)). \]
   
    \ExePart \label{II} Show that if 
    \begin{equation} \label{eq:alphstdef}\alpha^* = \arg\min_\alpha \frac{1}{2}(\alpha -v)^2+\lambda |\alpha|\end{equation}
    then 
    \begin{equation}\label{eq:l1inclusion}
    \alpha^* \in v - \lambda \partial |\alpha^*|.
    \end{equation}
    Note that by definition $\alpha^* = \mbox{prox}_{\lambda |\alpha|}(v)$.
      
     \ExePart \label{III} If $\lambda < v$ show that the solution to the inclusion~\eqref{eq:l1inclusion} is given by
     \[ \alpha^* = v-\lambda.\]
        
     \ExePart \label{IV} If $-\lambda < v < \lambda$ show that the solution to the inclusion~\eqref{eq:l1inclusion} is given by
     \[ \alpha^* = 0.\]
        
     \ExePart \label{V} Using the previous items, prove that
     \[\mbox{prox}_{\lambda |\alpha|} (v) = S_{\lambda}(v)  \]
     and that the equality~\eqref{eq:l1prox} holds. 
   \Answer[ref={I}] The trick lies in noticing that 
\begin{eqnarray*}
\min_x  \frac{1}{2}\norm{x-v}_2^2 + \sum_{i=1}^d f_i(x_i)
 &=&\min_x  \frac{1}{2}\sum_{i=1}^d (x_i-v_i)^2 + \sum_{i=1}^d f_i(x_i)\\
 &= & \sum_{i=1}^d \min_{x_i}   \frac{1}{2}(x_i-v_i)^2 +  f_i(x_i).
\end{eqnarray*} 
   Consequently
\begin{eqnarray*}
\arg \min_x  \frac{1}{2}\norm{x-v}_2^2 + f(x) &=&
   \left(\arg \min_{x_1} \left\{  \frac{1}{2}(x_1-v_1)^2 +  f_1(x_1)\right\}, \ldots, \arg\min_{x_d}   \left\{ \frac{1}{2}(x_d-v_d)^2 +  f_d(x_d)\right\} \right)\\   
   &= & \left(\mbox{prox}_{f_1}(v_1), \ldots,  \mbox{prox}_{f_d}(v_d)\right).
   \end{eqnarray*}

    \Answer[ref={II}]  The solution to~\eqref{eq:alphstdef} must be such that
    \[0 \in  \partial\left(\frac{1}{2}(\alpha^* -v)^2+\lambda |\alpha^*|\right)
    = \alpha^* -v + \lambda \partial |\alpha^*|.\]
    Rearranging the above gives $ \alpha^* \in v-  \lambda \partial |\alpha^*|.$
\Answer[ref={III}]     
     If $\lambda < v$ then  inclusion~\eqref{eq:l1inclusion} shows that
     \[\alpha^* \geq \lambda (1-\partial |\alpha^*|) \geq 0 \quad \Rightarrow \quad
   \partial |\alpha^*| = 1.  \]
   Consequently the set in~\eqref{eq:l1inclusion} then  inclusion~\eqref{eq:l1inclusion} shows that
   \[ \alpha^* = v - \lambda \partial |\alpha^*| = v-\lambda. \]
           
\Answer[ref={IV}]    If $-\lambda < v < \lambda$ then the solution to the inclusion~\eqref{eq:l1inclusion} is bounded by
    \begin{equation}\label{eq:ao989ond}
    \alpha^* \in v - \lambda \partial |\alpha^*| \quad \subset \quad\left]-\lambda (1+\partial |\alpha^*|), \,  \lambda (1-\partial |\alpha^*|) \right[.\end{equation}
     Now suppose that $\alpha^* <0$. The above shows that 
     \[\alpha^* \in \left] 0, \,  2\lambda  \right[, \]
     a contradiction. If $\alpha^* >0,$ then~\eqref{eq:ao989ond} shows that 
     \[ \alpha^* \in \left] -2\lambda, \,  0  \right[, \]
     another contraction. Finally if $\alpha^*$ then~\eqref{eq:ao989ond}  offers no contradiction since it is equivalent to 
          \[ \alpha^* \in \left] -\lambda, \,  \lambda  \right[. \]
     Consequently $-\lambda < v < \lambda \Rightarrow \alpha^* =0. $
     \Answer[ref={V}] Using analogous arguments to Ex~\ref{III} we can show that  $v < -\lambda \Rightarrow \alpha^* = v+ \lambda.$ By combing this observation together the solutions of Ex~\ref{III} and~\ref{IV} we have that
      \[\alpha^*= 
\begin{cases}
v- \lambda & \mbox{ if } \lambda <v\\
0 & \mbox{ if } -\lambda\leq v\leq \lambda\\
v+\lambda & \mbox{ if } v<  -\lambda.
\end{cases}\]
Thus 
\[\mbox{prox}_{\lambda |\alpha|}(v) = \alpha^* \overset{\eqref{eq:softth}}{ =} S_{\lambda}(v).\]
\end{ExerciseList}

 \section{Singular Value Soft Thresholding}
 Consider the extension of proximal operators to matrices
   \begin{equation}
  \mbox{prox}_{F}(A) \eqdef \arg\min_{X \in \R^{d\times d}} \frac{1}{2}\norm{X-A}_F^2 + F(X).
  \end{equation}
 We will now prove step by step that
  \begin{equation}\label{eq:l1prox}
  \mbox{prox}_{\lambda \norm{X}_*} (A) = U S_{\lambda}(\mbox{diag}(\sigma(A)))V^\top,
  \end{equation}
  where $\norm{X}_* = \sum_{i=1}^d \sigma_i(X)$ and $A=U\mbox{diag}(\sigma_i(A))V^\top$ is the singular value decomposition of $A.$
     
     This proximal operator forms the basis of the celebrated algorithm for solving the matrix completion problem~\cite{CaiCandes:2010}.
  \begin{ExerciseList}
    \Exercise
    

\ExePart \label{Is} Show that the nuclear and the Frobenius norm are invariant under rotations. That is, for any matrix $A$ and orthogonal matrices $O$ and $Q$ we have that
\[\norm{A}_F^2 = \norm{OA}_F^2 = \norm{AQ}_F^2\] 
and
\[\norm{A}_* = \norm{OA}_* = \norm{AQ}_*.\] 
    \ExePart  {\bf (Level HARD):} \label{IIs}   Prove that~\eqref{eq:l1prox} holds. You may use the following Theorem by Von Neumann
     \begin{theorem}[Von Neumann 1937]\label{theo:von} $\quad$
     For any matrices $X$ and $A$ of the same dimensions and orthogonal matrices $U$ and $V$, we have that
     \begin{equation}
\dotprod{UXV^\top, A} \leq \dotprod{\mbox{diag}(\sigma_i(X)), \mbox{diag}(\sigma_i(A))},
     \end{equation}
     where $\mbox{diag}(\sigma_i(A))$ is a diagonal matrix with the singulars values of $A$ on the diagonal.
     \end{theorem}    
  
       \Answer[ref={Is}] By the definition of Frobenius norm we have that
       \[\norm{OA}_F^2 = \Tr{A^\top (O^\top O) A} = \Tr{A^\top A} = \norm{A}_F^2.\]
       For the nuclear norm, note that for any orthogonal matrices $O$ and $U$ we have that $OU$ is an orthogonal matrix since 
       \[(OU)^\top OU = U^\top (O^\top O) U = U^\top U = I.\]
     Thus  is the SVD decomposition  is given by $A = U \mbox{diag}(\sigma_i(A)) V^\top$ then $OA = OU \mbox{diag}(\sigma_i(A)) V^\top$ is the SVD decomposition of $OA$, that is, the matrix $OA$ has the same singular values of $A$. Consequently we have that
       \[\norm{OA}_* = \norm{(OU) \mbox{diag}(\sigma_i(A)) V^\top}=
       \sum_{i=1}^d \sigma_i(A) = \norm{A}_*,
       \]
      by definition of nuclear norm.
    \Answer[ref={IIs}] 
    Substituting $A=U\mbox{diag}(\sigma_i(A))V^\top$ gives
    \[ \mbox{prox}_{\lambda \norm{X}_*} (A) = \arg\min_{X \in \R^{d\times d}} \frac{1}{2}\norm{U(U^\top XV-\mbox{diag}(\sigma_i(A)))V^\top}_F^2 + \lambda\norm{X}_*. \]
    Changing variable name 
    \begin{equation}\label{eq:XbarX}
    \bar{X} = U^\top XV,
    \end{equation} and noting that the Frobenius norm and the nuclear norm are invariant to orthogonal transforms we have that
 \begin{eqnarray}
\min_{X \in \R^{d\times d}} \frac{1}{2}\norm{U(\bar{X}-\mbox{diag}(\sigma_i(A)))V^\top}_F^2 + \lambda\norm{U\bar{X}V^\top}_* 
   = \min_{X \in \R^{d\times d}} \frac{1}{2}\norm{\bar{X}-\mbox{diag}(\sigma_i(A))}_F^2 + \lambda\norm{\bar{X}}_*.\label{eq:aniohnq88h}
\end{eqnarray}  
I now claim that  the solution $\bar{X}$ to the above must be a diagonal matrix. This is where Von Neumann's theorem comes into play. To see this let $U_x \mbox{diag}(\sigma_i(X))V_x^\top$ be the SVD decomposition of $X$. Thus
\begin{eqnarray*}
 \norm{\bar{X}-\mbox{diag}(\sigma_i(A))}_F^2 &=&
\norm{U_x \mbox{diag}(\sigma_i(X))V_x^\top-\mbox{diag}(\sigma_i(A))}_F^2 \\
& = & \norm{\mbox{diag}(\sigma_i(X))}_F^2 + \norm{\mbox{diag}(\sigma_i(A))}_F^2 -
2\dotprod{U_x \mbox{diag}(\sigma_i(X))V_x^\top, \mbox{diag}(\sigma_i(A))}\\
& \overset{\mbox{Theorem}~\ref{theo:von}}{\geq} &\norm{\mbox{diag}(\sigma_i(X))}_F^2 + \norm{\mbox{diag}(\sigma_i(A))}_F^2 -2 \dotprod{ \mbox{diag}(\sigma_i(X)), \mbox{diag}(\sigma_i(A))} \\
& =& \norm{\mbox{diag}(\sigma_i(X)) - \mbox{diag}(\sigma_i(A))}_F^2.
\end{eqnarray*}
Consequently
 \begin{eqnarray*}
\min_{\bar{X}}\frac{1}{2}\norm{\bar{X}
-\mbox{diag}(\sigma_i(A))}_F^2 + \lambda\norm{\bar{X}}_*
& \geq & \min_{\bar{X}}\frac{1}{2}\norm{\mbox{diag}(\sigma_i(\bar{X}))
-\mbox{diag}(\sigma_i(A))}_F^2 + \lambda\norm{\mbox{diag}(\sigma_i(\bar{X}))}_*,
\end{eqnarray*} 
where we used the invariance of the nuclear norm under orthogonal transformations.
This proves that the solution $\bar{X} = \mbox{diag}(\bar{X}_{11}, \ldots, \bar{X}_{dd})$ will be a diagonal matrix. From now on we assume that $\bar{X}= \mbox{diag}(\bar{X}_{ii})$ is a diagonal matrix.  Let $\bar{x} = (\bar{X}_{11}, \ldots, \bar{X}_{dd})$ be the vectorization of $\bar{X}.$
Thus $\norm{\bar{X}}_* = \norm{\bar{x}}_1$ and $\norm{\bar{X}}_F^2 = \norm{\bar{X}}_2^2.$ Let $\sigma(A) = [\sigma_1(A),\ldots, \sigma_d(A)]\in \R^d.$
Finally we have that~\eqref{eq:aniohnq88h} becomes
\[ \min_{X \in \R^{d\times d}} \frac{1}{2}\norm{\bar{X}-\mbox{diag}(\sigma_i(A))}_F^2 + \lambda\norm{\bar{X}}_* =
\min_{\bar{x} \in \R^{d}} \frac{1}{2}\norm{\bar{x}-\sigma(A)}_2^2 + \lambda \norm{\bar{x}}_1.  \]     
Consequently, taking the minimum argument we have that 
\[S_{\lambda}(\mbox{diag}(\sigma(A))) =
 \arg\min_{\bar{X} \mbox{ is diag}} \frac{1}{2}\norm{\bar{X}-\mbox{diag}(\sigma_i(A))}_F^2 + \norm{\bar{X}}_*,\]
where $S_{\lambda}(\mbox{diag}(\sigma(A))) := \mbox{diag}(S_{\lambda}(\sigma_i(A))).$ To conclude, note that our original argument is $UXV^\top = \bar{X}$ due to~\eqref{eq:XbarX}. Thus finally
\[   \mbox{prox}_{\lambda \norm{X}_*} (A) =  \arg\min_X \norm{X-A}_F^2 + \lambda \norm{X}_* = 
US_{\lambda}(\mbox{diag}(\sigma(A)))V^\top.\qed  \]

%\[\arg\min_{\bar{X} \mbox{ is diag}} \frac{1}{2}\norm{\bar{X}-\mbox{diag}(\sigma_i(A))}_F^2 + \norm{\bar{X}}_* =
%\arg\min_{\bar{x} \in \R^{d}} \frac{1}{2}\norm{\bar{x}-\sigma(A)}_2^2 + \norm{\bar{x}}_1 =\mbox{prox}_{\norm{x}_1}(\sigma(A)).  \]
\end{ExerciseList}
\printbibliography
\end{document}