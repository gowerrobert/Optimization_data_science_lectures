\documentclass[11pt]{article}


%\usepackage{kpfonts} % the kpfonts font
\usepackage{colordvi,epsfig,amssymb}
\usepackage{amsmath,graphicx,enumerate,geometry,array}

\usepackage[noanswer]{exercise}
%\usepackage{exercise}
\usepackage[firstinits=true,doi=false,isbn=false,url=false,backend=bibtex]{biblatex}	
\bibliography{../exe_conv}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\cqd}{\hfill\rule{2mm}{2mm}} % Full little box
\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

\newcommand{\eqdef}{\overset{\text{def}}{=}} 
\newcommand{\diag}{\mbox{diag}} 

\newcommand{\EE}[2]{\mathbb{E}_{#1}\left[#2\right] }
\newcommand{\E}[1]{\mathbb{E}\left[#1\right] } 
\newcommand{\Prb}[1]{\mathbb{P}\left[#1\right] }
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\dotprod}[1]{\left< #1\right>}
\newcommand{\Tr}[1]{\mbox{Tr}\left( #1\right)}
\providecommand{\Null}[1]{\mathbf{Null}\left( #1\right)}
\providecommand{\Rank}[1]{\mathbf{Rank}\left( #1\right)}
\providecommand{\Range}[1]{\mathbf{Range}\left( #1\right)}

\title{Exercise List: Properties and examples of convexity and smoothness}
\author{Robert M. Gower.}


\begin{document}
\maketitle
Time to get familiarized with convexity, smoothness and a bit of strong convexity. 
\paragraph{Notation:} For every $x, y, \in \R^d$ let $\dotprod{x,y} \eqdef x^\top y$ and let $\norm{x}_2 = \sqrt{\dotprod{x,x}}.$ 

Let $\sigma_{\min}(A)$ and $\sigma_{\max}(A)$ be the smallest and largest singular values of $A$ defined by
\begin{equation} \label{eq:sigvals}
\sigma_{\min}(A) \eqdef \min_{x \in \R^d} \frac{\norm{Ax}_2}{\norm{x}_2} \quad \mbox{and} \quad \sigma_{\max}(A) \eqdef \max_{x \in \R^d} \frac{\norm{Ax}_2}{\norm{x}_2} .
\end{equation}
Thus clearly
\begin{equation}\label{eq:induced}
\frac{\norm{Ax}_2^2}{\norm{x}_2^2} \leq \sigma_{\max}(A)^2, \quad \forall x \in \R^d.
\end{equation}
Let $\norm{A}_F^2 \eqdef \Tr{A^\top A}$ denote the Frobenius norm of $A.$ Finally, a result you will need, for every symmetric positive semi-definite matrix $G$ the $L2$ induced matrix norm can be equivalently defined by
\begin{equation} \label{eq:inducedG}
||G||_2 = \sigma_{\max}(G) = \sup_{x \in \R^d,\, x \neq 0} \frac{\dotprod{Gx,x}_2}{\norm{x}_2^2} = \max_{x \in \R^d,\, x \neq 0} \frac{\norm{Gx}_2}{\norm{x}_2}.
\end{equation}
  
  \section{Convexity}
We say that a twice differentiable function $f: \R^d \rightarrow \R$ is convex if 
\begin{equation} \label{eq:convoriginal}
f(\lambda x+(1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y), \quad \forall x,y \in \R^d, \lambda \in [0,\,1].
\end{equation}
or equivalently
\begin{equation} \label{eq:convhess}
v^\top \nabla^2 f(x) v \geq 0, \quad \forall x,v \in \R^d.
\end{equation}
We say that $f$ is $\mu$--strongly convex if
\begin{equation} \label{eq:strconvhess}
v^\top \nabla^2 f(x) v \geq \mu \norm{v}_2^2, \quad \forall x,v \in \R^d.
\end{equation}
\vspace{0.5cm}  
\begin{ExerciseList}
    \Exercise We say that $\norm{\cdot} \rightarrow \R_+$ is a norm over $\R^d$ if it satisfies the following three properties
 \begin{enumerate}
\item \hspace{0.5cm}{\bf Point separating:} $\norm{x} =0 \Leftrightarrow  x=0, \forall x \in \R^d.$\label{it:pntsep}
\item \hspace{0.5cm} {\bf Subadditive:} $\norm{x +y} \leq \norm{x}+\norm{y}, \forall x,y \in \R^d$ \label{it:subadd}
\item \hspace{0.5cm} {\bf Homogeneous:} $ \norm{ax} =|a|\norm{x}, \forall x \in \R^d, a \in \R.$ \label{it:homo}
 \end{enumerate}
\ExePart \label{I} Prove that $x \mapsto \norm{x}$ is a convex function.
  
 \ExePart \label{II} For every convex function $f: y\in \R^m \mapsto f(y),$ prove that $g: x\in \R^d \mapsto f(Ax-b) $ is a convex function, where $A \in \R^{m\times d}$ and $b \in \R^m.$ 
  
  \ExePart \label{V} Let $f_i:\R^d \rightarrow \R$ be convex for $i =1,\ldots, m.$ Prove that $\sum_{i=1}^mf_i$ is convex.
   
\ExePart \label{III} For  given scalars $y_i \in \R$ and vectors $a_i \in \R^d$ for $i=1,\ldots, m$ prove that the \emph{logistic regression} function $f(x) = \sum_{i=1}^m \ln(1+e^{-y_i\dotprod{x,a_i}})$ is convex.
 
\ExePart \label{IV} Let $A \in \R^{m \times d}$ have full column rank. Prove that $f(x) = \tfrac{1}{2}\norm{Ax -b}_2^2$ is $\sigma_{\min}^2(A)$--strongly convex.


   \Answer[ref={I}]
   Let $x,y \in \R^d$ and $\lambda \in [0,\, 1]$. It follows that
   \begin{eqnarray}
   \norm{\lambda x + (1-\lambda)y}  &\overset{\mbox{item}~\ref{it:subadd}}{\leq} &
   \norm{\lambda x} + \norm{(1-\lambda)y}\nonumber \\
    &\overset{\mbox{item}~\ref{it:homo}}{\leq} &   \lambda\norm{ x} + (1-\lambda)\norm{y}. \qed \nonumber
   \end{eqnarray}
   \Answer[ref={II}]    Let $x,y \in \R^d$ and $\lambda \in [0,\, 1]$. It follows that 
      \begin{eqnarray}
   g(\lambda x + (1-\lambda)y) & =&  f( A(\lambda x + (1-\lambda))y-b)\nonumber\\
   & =&  f( \lambda (A x-b) + (1-\lambda)(Ay-b)) \\
   & \overset{f \mbox{ is conv.}}{=}&  \lambda f(Ax-b) + (1-\lambda)f(Ay-b). \qed \nonumber
   \end{eqnarray}
     \Answer[ref={V}] Immediate through either definition.
    \Answer[ref={III}] From exercise~\ref{V} we need only prove  
 that $f(x) = \ln(1+e^{-y \dotprod{x,w}})$ is convex for a given $y \in \R$ and $w \in \R^d.$ From exercise~\ref{II} we need only prove that $\phi(\alpha) = \ln(1+e^{\alpha})$ is convex, since $x \mapsto -y \dotprod{x,w} $ is a linear function.  The convexity of $f(\alpha)$ now follows by differentiating once
\[\phi'(\alpha) = \frac{e^{\alpha}}{1+e^{\alpha}},\] 
 then differentiating again
\begin{equation}\label{eq:logistichess}
\phi''(\alpha) = \frac{e^{\alpha}}{1+e^{\alpha}}-\frac{e^{2\alpha}}{(1+e^{\alpha})^2}= \frac{e^{\alpha}}{(1+e^{\alpha})^2} \geq 0, 	\quad \forall \alpha.\end{equation}
 We can now call upon the definition~\eqref{eq:convhess}, but since $\alpha \in \R$ is a scalar, the above already proves that $\phi(\alpha)$ is convex.
%    \[\nabla f(x) = -w\frac{y}{1+e^{-y \dotprod{x,w}}},\]
%    and the second derivative is
%    \[\nabla f(x) = w w^\top \frac{y^2}{1+e^{-y \dotprod{x,w}}}.\]
%    Let $v \in \R^d$. Left multiplying the above by $v^\top$ and right multiplying by $v$ gives
%    \begin{equation}\label{eq:logistichess}
%     v^\top \nabla f(x) v = \dotprod{v,w}^2\frac{y^2}{1+e^{-y \dotprod{x,w}}} \geq 0.\end{equation}
     \Answer[ref={IV}]   Differentiating twice we have that 
     \[\nabla^2 f(x) = A^\top A.\]
     Consequently 
     \[v^\top \nabla^2 f(x)v = v^\top A^\top A v = \norm{Av}_2^2 \geq \sigma_{\min}(A)^2 \norm{v}_2^2.\]
\end{ExerciseList}

\section{Smoothness}
We say that a function $f: \R^d \rightarrow \R$ is $L$--smooth if 
\begin{eqnarray}\label{eq:smoothness}
\norm{\nabla f(x)- \nabla f(y)} &\leq & L \norm{x-y}
 \end{eqnarray}
or equivalently if $f$ is twice differentiable then
\begin{equation}\label{eq:smoothnesshess}
v^\top \nabla^2 f(x) v \leq L\norm{v}_2^2, \quad\forall x,v \in \R^d.
\end{equation}
\vspace{0.5cm}  
   
\begin{ExerciseList}
 \Exercise 
\ExePart \label{I} Prove that $x \mapsto \tfrac{1}{2}\norm{x}^2$ is $1$--smooth.
     
\ExePart~\label{VI}Let $f: \R^d \rightarrow \R$ be twice differentiable and $L$--smooth. Show that 
\[\sigma_{\max}(\nabla^2 f(x) ) = \norm{\nabla^2 f(x)}_2 \leq L.\]
    
 \ExePart \label{II} For every twice differentiable $L$--smooth function $f: y\in \R^m \mapsto f(y),$ prove that $g: x\in \R^d \mapsto f(Ax-b) $ is a smooth function, where $A \in \R^{m\times d}$ and $b \in \R^m.$ Find the smoothness constant of $g$.
   
\ExePart \label{III} Let $f_i:\R^d \rightarrow \R$ be a twice differentiable and $L_i$--smooth for $i =1,\ldots, m.$ Prove that $\tfrac{1}{n}\sum_{i=1}f_i$ is $\sum_{i=1}\tfrac{L_i}{n}$--smooth.

\ExePart \label{IV} For  given scalars $y_i \in \R$ and vectors $a_i \in \R^d$ for $i=1,\ldots, m$ prove that the \emph{logistic regression} function $f(x) = \tfrac{1}{m}\sum_{i=1}^m \ln(1+e^{-y_i\dotprod{x,a_i}})$ is smooth. Find the smoothness constant!
 
\ExePart \label{V} Let $A \in \R^{m \times d}$ be any matrix. Prove that $\norm{Ax -b}_2^2$ is  $\sigma_{\max}^2(A)$--smooth.
     
  \ExeText \emph{Hint 1:} ...\\
   \Answer[ref={I}]
   Clearly $\nabla^2 \tfrac{1}{2}\norm{x}^2 = I$ and thus follows from definition~\eqref{eq:smoothness}.
        
   \Answer[ref={VI}] Using the definition of the induced norm we have that
   \[ \norm{\nabla^2 f(x)}_2^2 = \sup_{v \neq 0} \frac{v^\top \nabla^2 f(x) v}{\norm{v}_2^2}
   \overset{\eqref{eq:smoothnesshess}}{ \leq} 
   \sup_{v \neq 0} \frac{L\norm{v}_2^2}{\norm{v}_2^2} =L. \]
        
   \Answer[ref={II}]    Differentiating $g(x)$ once gives
   \[\nabla g(x) = A^\top \nabla f(Ax-b).\]
First we prove the claim using the definition~\eqref{eq:smoothness}. Indeed note that
\begin{eqnarray*}
\norm{\nabla g(x) - \nabla g(y)}_2 & = &
\norm{A^\top \left(\nabla f(Ax-b) - \nabla f(Ay-b)\right)}_2 \\
& \leq & \norm{A^\top}_2 \norm{\nabla f(Ax-b) - \nabla f(Ay-b)}_2 \\
& \overset{\mbox{smooth. of $f$}}{\leq} &L\norm{A^\top}_2 \norm{Ax-b -(Ay -b)}_2 \\
& \leq & L\norm{A^\top}_2\norm{A}_2  \norm{x-y}_2.
\end{eqnarray*}   
   This the smoothness parameter is given by $L\norm{A}_2^2$ where we used that $\norm{A^\top}_2 = \norm{A}_2.$ This completes the proof.
   
   
  We can also prove the claim using~\eqref{eq:smoothnesshess}. Differentiating again we have that
   \[\nabla^2 g(x) = A^\top  \nabla^2 f(Ax-b)A.\]  
   Consequently
   \[\norm{ \nabla^2 g(x)}_2^2\leq \norm{A}_2^2 \norm{\nabla^2 f(Ax-b)}_2^2 \leq L\norm{A}_2^2. \]   
    We could further tighten this by considering the smoothness constant of $f$ restricted to the set $\{x \, | \, Ax-b\}$ which might be smaller then $\R^d.$
 \Answer[ref={III}] Clearly
    \[ \nabla^2 (\tfrac{1}{n} \sum_{i=1}^n f_i(x)) =
    \tfrac{1}{n} \sum_{i=1}^n \nabla^2f_i(x) \preceq \tfrac{1}{n} \sum_{i=1}^n L_i I.  \]
    You can also prove this using the definition~\eqref{eq:smoothness} and applying repeatedly the subadditivity of the norm.
 \Answer[ref={IV}] First note that from\eqref{eq:logistichess} the function  $\phi(\alpha) = \ln(1+e^{\alpha})$ is $1$--smooth. Consequently from exercise~\ref{II} the function $f_i(x) = \ln(1+e^{-y_i\dotprod{x,a_i}})$ is $y_i^2\norm{a_i}_2^2$--smooth. Finally from exercise~\ref{III} the logistic regression function is $\sum_{i=1}^m \frac{y_i^2\norm{a_i}_2^2}{m}$--smooth.
 
% 
% that the function 
% Since the logistic regression function is an average of functions, and these functions are in turn compositions with a linear transform. 
% 
% From~\eqref{eq:logistichess} we have that
% \[     v^\top \nabla f(x) v =\dotprod{v,w}^2\frac{y^2}{1+e^{-y \dotprod{x,w}}}  \leq  \dotprod{v,w}^2y^2 \leq (y^2 \norm{w}_2^2 )\norm{v}_2^2.\]
% Thus the logistic regression function is $y^2 \norm{w}_2^2$ smooth. We need only take the sum over different logistic functions.
  \Answer[ref={V}]     Differentiating twice we have that 
     \[\nabla^2 f(x) = A^\top A.\]
     Consequently 
     \[v^\top \nabla^2 f(x)v = v^\top A^\top A v \leq \norm{Av}_2^2 \leq \sigma_{\max}(A)^2 \norm{v}_2^2.\]
\end{ExerciseList}
\printbibliography
\end{document}